{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import tarfile\n",
    "import urllib\n",
    "import shutil\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from xml.etree import ElementTree as ET\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "urls = ['http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz',\n",
    "        'http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(download_dir):\n",
    "    for url in urls:\n",
    "        target_file = url.split('/')[-1]\n",
    "        if target_file not in os.listdir(download_dir):\n",
    "            print(f'Downloading {target_file} ...')\n",
    "            urllib.request.urlretrieve(url, os.path.join(download_dir, target_file))\n",
    "        else:\n",
    "            print(f'Already downloaded {target_file}')\n",
    "\n",
    "def extract(data_dir, download_dir):\n",
    "    for url in urls:\n",
    "        target_file = url.split('/')[-1]\n",
    "        target_dir = target_file.replace('.tar.gz', '')\n",
    "        assert target_file in os.listdir(download_dir), f'{target_file} not found in {download_dir}'\n",
    "        if target_dir not in os.listdir(data_dir):\n",
    "            print(f'Extracting {target_file} ...')\n",
    "            tf = tarfile.open(url.split('/')[-1])\n",
    "            tf.extractall(data_dir)\n",
    "        else:\n",
    "            print(f'Already extracted {target_file}')\n",
    "\n",
    "\n",
    "Path('data').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "download('.')\n",
    "extract('data', '.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data\n",
    "\n",
    "Even thought the \"annotations\" directory have more masks (`.png`) than the actual images (`.jpg`) in the \"images\" directory, we'll only use the ones we need (the ones we have the `.jpg` for).\n",
    "\n",
    "The structure of the files is like `image_name.jpg` and then the mask with the same name but with a `.png` extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trimaps_dir = 'data/annotations/trimaps/'\n",
    "\n",
    "maps = [x for x in os.listdir(trimaps_dir) if x[-3:] == 'png']\n",
    "print(f\"num of masks: {len(maps)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'data/images/'\n",
    "\n",
    "images = [x for x in os.listdir(image_dir) if x[-3:] == 'jpg']\n",
    "print(f\"num of images:{len(images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images[5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i in range(0, 4):\n",
    "    index = random.randint(0, len(images) - 1)\n",
    "    image_name = images[index]\n",
    "    map_name = images[index].split('.')[0] + '.png'\n",
    "\n",
    "    plt.subplot(4, 2, 1 + i*2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(plt.imread(os.path.join(trimaps_dir, map_name)))\n",
    "    plt.subplot(4, 2, 2 + i*2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(plt.imread(os.path.join(image_dir, image_name)))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of classes in the mask\n",
    "# a class for background, a class for the object (forground), and a class for the object boundary (or also known as niether region)\n",
    "img = Image.open(os.path.join(trimaps_dir, maps[0]))\n",
    "print(np.unique(img))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sagemaker role\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# A bucket is a fundamental storage container within AWS Simple Storage Service (S3).\n",
    "# A bucket provides a secure and scalable way to store and retrieve any amount of data from anywhere on the web.\n",
    "# You need to create the S3 bucket (from the AWS website) before you can use it.\n",
    "# note that the bucket name must be unique across all AWS accounts (not just your account)!\n",
    "bucket_name = \"petdatamy\"\n",
    "\n",
    "# we'll be using the sagemaker's built-in semantic segmentation algorithm\n",
    "# the algorithm is a pre-built container that is hosted on Amazon Elastic Container Registry (ECR)\n",
    "# the `training_image`, refers to the URI of the container image that contains the algorithm\n",
    "# `boto3.Session().region_name` is the region where the algorithm is hosted\n",
    "training_image = sagemaker.image_uris.retrieve(\n",
    "    framework='semantic-segmentation', \n",
    "    region=boto3.Session().region_name, \n",
    "    version='latest',\n",
    "    )\n",
    "\n",
    "print(f\"Training image: {training_image}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train -> holds training images\n",
    "# validation -> holds validation images\n",
    "\n",
    "# train_annotation -> holds training trimaps (masks)\n",
    "# validation_annotation -> holds validation trimaps (masks)\n",
    "\n",
    "pre = Path('local_bucket')\n",
    "folders = ['train', 'train_annotation', 'validation', 'validation_annotation']\n",
    "\n",
    "for folder in folders:\n",
    "    folder = pre / folder\n",
    "    # uncomment to delete local_bucket\n",
    "    shutil.rmtree(folder) if folder.exists() else None\n",
    "    folder.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data for SageMaker\n",
    "We move the dataset to AWS Bucket for the SageMaker to work on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_file(image):\n",
    "    \"\"\"get the trimap for a given image\"\"\"\n",
    "    map_file = image.split('.')[0] + '.png'\n",
    "    assert map_file in maps\n",
    "    return map_file\n",
    "\n",
    "\n",
    "# if images exist in the local bucket, we skip the loop to prevent a messy copy\n",
    "# delete the local bucket if you want to split and copy all images again\n",
    "if len(set((pre / 'train').iterdir())) > 0:\n",
    "    print(f'Images exists in {pre}')\n",
    "else:\n",
    "    for image in tqdm(images):\n",
    "        # randomly assign 75% of the images to the training set and 25% to the validation set\n",
    "        # since we are looping over each image once, there is no need to prevent duplicates\n",
    "        # note that this random assignment doesn't guarantee exactly 75% / 25% split!\n",
    "        # It's probabilistic, so we might get slightly different proportions each time the code runs\n",
    "        target_set = 'train' if random.randint(0, 99) < 75 else 'validation'\n",
    "        \n",
    "        # constructing the image path\n",
    "        image_file_path = Path('data/images') / image\n",
    "        image_target_path = pre / target_set / image  # for the local bucket\n",
    "        \n",
    "        # constructing the trimap path\n",
    "        map_file_path = Path(trimaps_dir) / get_map_file(image)\n",
    "        map_target_path = pre / (target_set + '_annotation') / get_map_file(image)  # for the local bucket\n",
    "        \n",
    "        # copying the images to the target directories\n",
    "        shutil.copy(image_file_path, image_target_path)\n",
    "        shutil.copy(map_file_path, map_target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = set((pre / 'train').iterdir())\n",
    "train_annots = set((pre / 'train_annotation').iterdir())\n",
    "\n",
    "print(f\"{len(train_annots)=}, \\n{len(train_images)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading Data to S3\n",
    "\n",
    "Technically, S3 doesn't have any folder structure.\n",
    "It uses a flat namespace to store the objects.\n",
    "However, the S3 console displays the objects in a folder-like structure by using prefixes.\n",
    "when we specify a prefix, S3 will create a virtual folder structure for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "\n",
    "upload = False  # set to True to upload the data to S3\n",
    "\n",
    "if upload:\n",
    "    print(\"Starting uploading to S3 ...\")\n",
    "    print(\"Uploading train images ...\")\n",
    "\n",
    "    s3_train_path = sess.upload_data(path=pre/'train', bucket=bucket_name, key_prefix='train') \n",
    "\n",
    "    print(\"Uploading train annotation ...\")\n",
    "    s3_train_annotation_path = sess.upload_data(path=pre/'train_annotation', bucket=bucket_name, key_prefix='train_annotation')\n",
    "\n",
    "    print(\"Uploading validation images ...\")\n",
    "    s3_val_path = sess.upload_data(path=pre/'validation', bucket=bucket_name, key_prefix='validation')\n",
    "\n",
    "    print(\"Uploading validation annotation ...\")\n",
    "    s3_val_annotation_path = sess.upload_data(path=pre/'validation_annotation', bucket=bucket_name, key_prefix='validation_annotation')\n",
    "\n",
    "    print(f\"Finished uploading to S3\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s3_train_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_api = sagemaker.estimator.Estimator(\n",
    "    training_image,\n",
    "    role = role,\n",
    "    sagemaker_session=sess,\n",
    "\n",
    "    instance_count=1,  # number of GPUs\n",
    "    \n",
    "    # note that the instance type must be compatible with the algorithm\n",
    "    instance_type='ml.p3.2xlarge', # 16 GB GPU machine\n",
    "\n",
    "    # The size of the EBS (Elastic Block Store) volume that will be attached to the instance\n",
    "    volume_size=100,  # in GB\n",
    "\n",
    "    # Upper limit of the time that the model can run for\n",
    "    max_run=36_000,  # in seconds\n",
    "\n",
    "    # the mode that the input data is stored in the bucket\n",
    "    input_mode='File',\n",
    "\n",
    "    output_path=f's3://{bucket_name}/output',  # where the trained model artifacts will be stored\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters\n",
    "\n",
    "**What is Pacemaker?**\n",
    "\n",
    "an open-source high availability (HA) resource manager that AWS uses to orchestrate and manage the availability of SAP applications and databases, ensuring minimal downtime in case of failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_api.set_hyperparameters(\n",
    "    backbone='resnet50',\n",
    "    use_pretrained_model=True,  # pretrained on the ImageNet dataset\n",
    "    algorithm='fcn', # Fully Convolutional Network\n",
    "    crop_size=240,  # the size of the input images\n",
    "    \n",
    "    # the number of classes in the dataset.\n",
    "    # eventhough we have 3 classes, our trimaps classes are not 0-2 but 1-3\n",
    "    # so we have 2 options: either we change all the trimap pngs to 0-2, or we set the num_classes to 4\n",
    "    # and hope the algorithm will learn to ignore class zero.\n",
    "    num_classes=4,\n",
    "    num_training_samples=len(train_images),\n",
    "\n",
    "    epochs=10,\n",
    "    learning_rate=0.001,\n",
    "    optimizer='rmsprop', # adam, sgd, or rmsprop\n",
    "    lr_scheduler='poly',  # step, cosine, or poly\n",
    "    # weight_decay=0.0001,\n",
    "\n",
    "    mini_batch_size=16,\n",
    "    validation_mini_batch_size=16,\n",
    "\n",
    "    # early_stopping=True,\n",
    "    # early_stopping_patience=5,\n",
    "    # early_stopping_metric='validation:loss',\n",
    "    # early_stopping_metric_criteria='min',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Channels\n",
    "\n",
    "These will point to the data locations in \"S3\". We pass this to the model to know where to look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sagemaker.session.s3_input(\n",
    "    s3_train_path,\n",
    "    distribution='FullyReplicated',  # the data is replicated across all instances if we are distribute training\n",
    "    content_type='image/jpeg',\n",
    "    # `S3Prefix` will look at the files in the directory and create the training data channel\n",
    "    s3_data_type='S3Prefix',  # or S3DataTypeManifest\n",
    ")\n",
    "\n",
    "train_ann_data = sagemaker.session.s3_input(\n",
    "    s3_train_annotation_path,\n",
    "    distribution='FullyReplicated',\n",
    "    content_type='image/png',\n",
    "    s3_data_type='S3Prefix',\n",
    ")\n",
    "\n",
    "val_data = sagemaker.session.s3_input (\n",
    "    s3_val_path,\n",
    "    distribution='FullyReplicated',\n",
    "    content_type='image/jpeg',\n",
    "    s3_data_type='S3Prefix',\n",
    ")\n",
    "\n",
    "val_ann_data = sagemaker.session.s3_input(\n",
    "    s3_val_annotation_path,\n",
    "    distribution='FullyReplicated',\n",
    "    content_type='image/png',\n",
    "    s3_data_type='S3Prefix',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_channels = {\n",
    "    'train': train_data,\n",
    "    'train_annotation': train_ann_data,\n",
    "    'validation': val_data,\n",
    "    'validation_annotation': val_ann_data\n",
    "}\n",
    "\n",
    "print(data_channels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'validation'\n",
    "images = [x for x in os.listdir(image_dir) if x[-3:] == 'jpg']\n",
    "print(len(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 2\n",
    "\n",
    "image_path = os.path.join(image_dir, images[index])\n",
    "# image_path = 'dog_cat.jfif'\n",
    "\n",
    "with open(image_path, 'rb') as f:\n",
    "    b = bytearray(f.read())\n",
    "\n",
    "results = deployed_model.predict(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array(Image.open(io.BytesIO(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread(image_path));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget!! You need to delete endpoint or else you will continue to accrue cost!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(deployed_model.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
